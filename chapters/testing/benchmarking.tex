\section{Benchmarking methodology}\label{section:benchmarking}

A companion testing class \code{Benchmark} has been created to provide a standard testing interface across different tested implementations. A tested solution is wrapped in a lambda function, accepting a timer instance and an input sequence as its arguments. This lambda function is invoked ten times, and the results are averaged before storing on disk for further processing.

The \code{Timer} class will capture the time spent between the invocations of \code{start()} and \code{stop()}. Internally, CPU based implementations are  measured using \code{std::chrono::high\_resolution\_clock}, whereas GPU based solutions will use the built-in CUDA runtime API.

In most scenarios, the time spent on copying the data between the CPU and GPU were excluded in the actual measurement.

\subsection{Datasets used in benchmarking}\label{subsection:benchmark:datasets}

Different data sequences were generated as the input data for benchmarking created implementations and existing solutions. Assuming $n$ is the expected size of generated data sequence, these datasets were used:

\begin{itemize}
  \item \textit{Ascending} -- sorted sequence $0..(n-1)$,
  \item \textit{Descending} -- inverse of ascending dataset, a sorted sequence $(n-1)..0$,
  \item \textit{Almost sorted} -- sorted sequence $0..(n-1)$ with 5 random swaps,
  \item \textit{Shuffle} -- generated sequence shuffled with \code{std::shuffle},
  \item \textit{Gaussean} -- a sequence generated by producing $n$ random values around mean with standard deviation.
\end{itemize}

For all of the measurements done in Section \ref{section:results}, 32-bit values are used both as keys and values.

As a comparison between the CPU and the GPU implementation

\subsection{Chosen implementations for comparison}\label{subsection:benchmark:implementations}

As for the implementations themselves, the following projects were chosen for the benchmarks:

\begin{itemize}
  \item \textit{OWG} (owensgroup/GpuBTree) -- implementation of a GPU based B-Link-Tree by Awad et al. \cite{awad},
  \item \textit{PALM} (runshenzhu/palmtree) -- an implementation of PALM Tree \cite{palm} by Xian et al. \cite{palm-impl}; a concurrent lock-free B+Tree scaling up to 16 cores,
  \item \textit{TLX} -- a collection of \CC\,data structures, algorithms and helprs by Bingman et al. \cite{TLX}.
\end{itemize}

\code{std::map} is chosen as the baseline when calculating the performance speedup. Internally, \code{std::map} tends to be implemented as a self-balanced red-black tree \cite{cppreference-map}.

Additional implementations, such as the Bw-Tree \cite{bw-tree} implementation by Wang et al. \cite{bwtree-impl}, or STX B$^+$Tree by Timo Bingman \cite{stx-b+tree}, were considered as well. But either due to issues or deprecation by the author, these solutions were not included in the comparison. Nevertheless, the obtained results can be found in the attached medium.

As the OWG implementation is the only readily available GPU implementation of the B-tree, additional work has been done to resolve issues encountered while benchmarking. Patches were created for the OWG implementation to mitigate memory leaks. These patches can be found in the benchmarking source code and will be automatically applied when CMake is invoked.

\section{Results}\label{section:results}

Chosen implementations described in Section \ref{subsection:benchmark:implementations} were measured against different sequences mentioned in Section \ref{subsection:benchmark:datasets}.

For the speedup comparison, \code{std::map} is chosen as a baseline for querying and inserting 32-bit key-value pairs. First column indicate the size of the sequence used. Second column contains the raw execution time spent by \code{std::map} on a benchmark test. Remaining speedup values are presented as the ratio of the execution time spent by \code{std::map} to the execution time spent by a tested implementation:

$$\mathit{Speedup} = S = \frac{\tau_{\mathit{CPU}}}{\tau_{\mathit{GPU}}}$$

\subsection{Query performance}
\begin{table}[h]
  \centering
  \input{components/table/query-shuffle.tex}
  \caption{Key-value searching speed-up of chosen implementations compared to \code{std::map} for various input sizes. \textit{Shuffled} sequence is used as input.}
  \label{table:query-shuffle}
\end{table}

In Table \ref{table:query-shuffle} the query speedup on \textit{shuffled} sequence is shown. For smaller sequences, \code{std::map} tends to be faster than other compared implementations. However, as the input size increases, the solutions utilizing parallelization overtake the baseline implementation.

As soon as the input size is larger than $2^{10}$, both the TNL$^+$ and TNL$^{link}$ becomes faster than the baseline \code{std::map}. A notable observation can be made when comparing TNL$^+$ (implementation of B$^+$Tree) with TNL$^{link}$ (implementation of B-Link-Tree). Even though the concurrency control is better in TNL$^{link}$ than in TNL$^+$, the reduced instruction count and reduced pointer chasing through sibling links overshadow the performance benefits of such concurrency control, resulting in similar speedup on \textit{shuffled} and \textit{gaussian} input sequence.

\begin{table}[h]
  \centering
  \input{components/table/query-increasing.tex}
  \caption{Key-value searching speed-up of chosen implementations compared to \code{std::map} for various input sizes. \textit{Ascending} sequence is used as input.}
  \label{table:query-increasing}
\end{table}

\begin{figure}[H]
  \input{components/figure/query-gaussian.pgf}
  \caption{Graph comparing number of search queries per second between chosen implementations for various input sizes. \textit{Gaussian} distribution is used as input.}
  \label{figure:query-gaussian}
\end{figure}

A different behavior can be observed in Table \ref{table:query-increasing},where the query speedup is shown on \textit{ascending} input sequence. The performance of the CPU implementation have significantly improved, thus reducing the measured query speedup on the GPU implementations.

In all input distributions, the OWG implementation of GPU B-Tree performs $2\times$ to $3\times$ faster than both the TNL$^{link}$ and TNL$^+$, regardless of input size, which can be seen in Figure \ref{figure:query-gaussian}. Internally, OWG utilizes explicit memory read and write operations with hand-crafted \code{asm} statements, where each thread in a warp reads a 32-bit word from global memory, instead of relying on implicit read instructions generated by the compiler. This approach yields better memory utilization in exchange for loss of generality. On a related note, even with applied patches, the OWG implementation suffers from frequent crashes caused by invalid memory accesses and deadlocks for input sequences larger than $2^{22}$, thus the OWG results are missing for these input sizes.

\subsection{Insertion performance}

\begin{table}[H]
  \centering
  \input{components/table/insert-shuffle.tex}
  \caption{Insertion speed-up of chosen implementations compared to \mintinline{cpp}{std::map} for various input sizes. \textit{Shuffled} sequence is used as input.}
  \label{table:insert-shuffle}
\end{table}

In \cref{table:insert-shuffle} it can be seen, that \mintinline{cpp}{std::map} is generally not performant even on a single thread, as both TLX and TNL$^{host}$ overtake the baseline when inserting more than $2^{15}$ items.


\begin{figure}
  \input{components/figure/insert-gaussian.pgf}
  \caption{Graph comparing the number of inserted keys-value pairs per second between chosen implementations for various input sizes. \textit{Gaussian} distribution is used as input.}
  \label{figure:insert-gaussian}
\end{figure}

\begin{figure}
  \input{components/figure/insert-increasing.pgf}
  \caption{Graph comparing the number of inserted keys-value pairs per second between chosen implementations for various input sizes. \textit{Increasing} sequence is used as input.}
  \label{figure:insert-increasing}
\end{figure}

\clearpage

One notable observation can be gathered from the results of PALM tree, as the insertion time is comparably the same as querying time; a result more in line with other CPU implementations. All GPU based implementations are slower than PALM when inserting when inserting less than $2^{17}$ items. This performance is consistent regardless of the distribution of incoming sequence, as seen in \cref{figure:insert-gaussian,figure:insert-increasing}, where PALM is the most performant implementation in the scenario of \textit{increasing} input sequence.

The performance difference between OWG and TNL implementations is similar compared to the previous query benchmarks with \textit{shuffled} input sequence, where TNL structures are generally $\approx$ 3$\times$ slower than OWG. When inserting key-value pairs in a sorted manner, as seen  in \cref{figure:insert-increasing}, OWG becomes slower than both of the TNL implemenations, when inserting $2^{{17}}$ items and more.
