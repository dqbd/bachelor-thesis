\section{Benchmarking methodology}\label{section:benchmarking}

Companion benchmarking methods have been created to provide a standard interface across different implementations of our choice. A tested solution is wrapped in a lambda function, accepting a timer instance and an input sequence as its arguments. This lambda function is invoked ten times, and an arithmetic mean of captured results is computed and stored on a disk. An example code can be seen in \cref{lst:benchmark} and the source code of the benchmarking methods can be found in \code{/benchmark/\_common/benchmark.hpp}.

\begin{listing}
  \begin{minted}[linenos]{cpp}
using DeviceType = Benchmark::Device::Host;
using NumericType = uint32_t;
Benchmark::execute<DeviceType, NumericType>(
  "test", [](Benchmark::BenchTimer<DeviceType> &timer,
              const std::vector<NumericType> &input) {
    timer.start();
    // insert operation
    timer.stop("insert");

    timer.start();
    // query operation
    timer.stop("query");
  });
  \end{minted}
  \caption{Sample usage of the benchmark methods found in \code{benchmark.hpp}.}
  \label{lst:benchmark}
\end{listing}

The \mintinline{cpp}{Benchmark::BenchTimer} class will capture the time spent between the invocations of \mintinline{cpp}{start()} and \mintinline{cpp}{stop()}. This class does support multiple timers within a single instance by specifying a key string in the stop function, as seen in line 8 and 12 of \cref{lst:benchmark}. Internally, CPU based implementations are measured using \mintinline{cpp}{std::chrono::high_resolution_clock}, whereas GPU based solutions will use the built-in CUDA runtime API, which is more p

In most scenarios, the time spent on copying the data between the CPU and GPU were excluded in the actual measurement.

\subsection{Datasets used in benchmarking}\label{subsection:benchmark:datasets}

Different data sequences were generated as the input data for benchmarking created implementations and existing solutions. Assuming $n$ is the expected size of generated data sequence, these datasets were used:

\begin{itemize}
  \item \textit{Ascending} -- sorted sequence $0..(n-1)$,
  \item \textit{Descending} -- inverse of ascending dataset, a sorted sequence $(n-1)..0$,
  \item \textit{Almost sorted} -- sorted sequence $0..(n-1)$ with 5 random swaps,
  \item \textit{Shuffle} -- generated sequence shuffled with \code{std::shuffle},
  \item \textit{Gaussean} -- a sequence generated by producing $n$ random values around mean with standard deviation.
\end{itemize}

For all of the measurements done in \cref{section:results}, 32-bit values are used both as keys and values, as some implementations chosen in \cref{subsection:benchmark:implementations} do not function properly when different data types are utilized. For this reason, no benchmark tests utilizing non-integer keys or values, required for comparison on datasets from unstructured numerical meshes, are presented here. As an example of such test implemented for TNL based implementations can be found in the source code (\code{/benchmark/tnl/tnl\_mesh\_cuda.cu}).

\subsection{Chosen implementations for comparison}\label{subsection:benchmark:implementations}

As for the implementations themselves, the following projects were chosen for the benchmarks:

\begin{itemize}
  \item \textit{OWG} (owensgroup/GpuBTree) -- implementation of a GPU based B-Link-Tree by Awad et al. \cite{awad},
  \item \textit{PALM} (runshenzhu/palmtree) -- an implementation of PALM Tree \cite{palm} by Xian et al. \cite{palm-impl}; a concurrent lock-free B+Tree scaling up to 16 cores,
  \item \textit{TLX} -- a collection of \CC\,data structures, algorithms and helprs by Bingman et al. \cite{TLX}.
\end{itemize}

\code{std::map} is chosen as the baseline when calculating the performance speedup. Internally, \code{std::map} tends to be implemented as a self-balanced red-black tree \cite{cppreference-map}.

Additional implementations, such as the Bw-Tree \cite{bw-tree} implementation by Wang et al. \cite{bwtree-impl}, or STX B$^+$Tree by Timo Bingman \cite{stx-b+tree}, were considered as well. But either due to issues or deprecation by the author, these solutions were not included in the comparison. Nevertheless, the obtained results can be found in the attached medium.

As the OWG implementation is the only readily available GPU implementation of the B-tree, additional work has been done to resolve issues encountered while benchmarking. Patches were created for the OWG implementation to mitigate memory leaks. These patches can be found in the benchmarking source code and will be automatically applied when CMake is invoked.

\section{Results}\label{section:results}

Chosen implementations described in \cref{subsection:benchmark:implementations} were measured against different sequences mentioned in \cref{subsection:benchmark:datasets}.

For the speedup comparison, \code{std::map} is chosen as a baseline for querying and inserting 32-bit key-value pairs. First column indicate the size of the sequence used. Second column contains the raw execution time spent by \code{std::map} on a benchmark test. Remaining speedup values are presented as the ratio of the execution time spent by \code{std::map} to the execution time spent by a tested implementation:

$$\mathit{Speedup} = S = \frac{\tau_{\mathit{CPU}}}{\tau_{\mathit{GPU}}}$$

\subsection{Query performance}
\begin{table}
  \centering
  \input{components/table/query-shuffle.tex}
  \caption{Key-value searching speed-up of chosen implementations compared to \code{std::map} for various input sizes. \textit{Shuffled} sequence is used as input.}
  \label{table:query-shuffle}
\end{table}

In \cref{table:query-shuffle} the query speedup on \textit{shuffled} sequence is shown. For smaller sequences, \code{std::map} tends to be faster than other compared implementations. However, as the input size increases, the solutions utilizing parallelization overtake the baseline implementation.

As soon as the input size is larger than $2^{10}$, both the TNL$^+$ and TNL$^{link}$ becomes faster than the baseline \code{std::map}. A notable observation can be made when comparing TNL$^+$ (implementation of B$^+$Tree) with TNL$^{link}$ (implementation of B-Link-Tree). Even though the concurrency control is better in TNL$^{link}$ than in TNL$^+$, the reduced instruction count and reduced pointer chasing through sibling links overshadow the performance benefits of such concurrency control, resulting in similar speedup on \textit{shuffled} and \textit{gaussian} input sequence.

\begin{table}
  \centering
  \input{components/table/query-increasing.tex}
  \caption{Key-value searching speed-up of chosen implementations compared to \code{std::map} for various input sizes. \textit{Ascending} sequence is used as input.}
  \label{table:query-increasing}
\end{table}

\begin{figure}
  \input{components/figure/query-gaussian.pgf}
  \caption{Graph comparing number of search queries per second between chosen implementations for various input sizes. \textit{Gaussian} distribution is used as input.}
  \label{figure:query-gaussian}
\end{figure}

Different behavior can be observed in \cref{table:query-increasing}, where the query speedup is shown on \textit{ascending} input sequence. The performance of the CPU implementation has significantly improved, thus reducing the measured query speedup on the GPU implementations.

In all input distributions, the OWG implementation of GPU B-Tree performs $2\times$ to $3\times$ faster than both the TNL$^{link}$ and TNL$^+$, regardless of input size, which can be seen in \cref{figure:query-gaussian}. Internally, OWG utilizes explicit memory read and write operations with hand-crafted \code{asm} statements, where each thread in a warp reads a 32-bit word from global memory instead of relying on implicit read instructions generated by the compiler. This approach yields better memory utilization in exchange for the loss of generality. On a related note, even with applied patches, the OWG implementation suffers from frequent crashes caused by invalid memory accesses and deadlocks for input sequences larger than $2^{22}$. Thus the OWG results are missing for these input sizes.

\subsection{Insertion performance}

\begin{table}
  \centering
  \input{components/table/insert-shuffle.tex}
  \caption{Insertion speed-up of chosen implementations compared to \mintinline{cpp}{std::map} for various input sizes. \textit{Shuffled} sequence is used as input.}
  \label{table:insert-shuffle}
\end{table}

In \cref{table:insert-shuffle} it can be shown, that \mintinline{cpp}{std::map} is generally not performant even on a single thread, as both TLX and TNL$^{host}$ overtake the baseline when inserting more than $2^{15}$ items.


\begin{figure}
  \input{components/figure/insert-gaussian.pgf}
  \caption{Graph comparing the number of inserted keys-value pairs per second between chosen implementations for various input sizes. \textit{Gaussian} distribution is used as input.}
  \label{figure:insert-gaussian}
\end{figure}

\begin{figure}
  \input{components/figure/insert-increasing.pgf}
  \caption{Graph comparing the number of inserted keys-value pairs per second between chosen implementations for various input sizes. \textit{Increasing} sequence is used as input.}
  \label{figure:insert-increasing}
\end{figure}

\clearpage

One notable observation can be gathered from the results of the PALM tree, as the insertion time is comparably the same as the querying execution time, a result more in line with other CPU implementations. All GPU-based implementations are slower than PALM when inserting less than $2^{17}$ items. This performance is consistent regardless of the distribution of incoming sequence, as seen in \cref{figure:insert-gaussian,figure:insert-increasing}, where PALM is the most performant implementation in the scenario of \textit{increasing} input sequence.

The performance difference between OWG and TNL implementations is similar compared to the previous query benchmarks with \textit{shuffled} input sequence, where TNL structures are generally $\approx$ 3$\times$ slower than OWG. When inserting key-value pairs in a sorted manner, as seen in \cref{figure:insert-increasing}, OWG becomes slower than both of the TNL implementations when inserting $2^{{17}}$ items and more.
