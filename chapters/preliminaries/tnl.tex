\section{TNL}

\acrfull{tnl} \cite{tnl} is a \CC\ library offering robust tools for high-performance computing and computational science. The primary goal of this project is to provide a familiar API akin to \code{STL} while offering significant performance uplift by exploiting the parallel nature of GPUs.

The library makes extensive use of template meta-programming to create a unified interface for both \acrshort{cpu} and \acrshort{gpu}, which stays the same regardless of the selected execution target device. This interface allows the programmer to enable or disable invocation on GPU without significant rewrites. This programming pattern heavily influenced the design of the implementation.

\subsection{Views}

As mentioned in \cref{label:gpu:mem}, \acrshort{gpu} have their own separate memory and address space, which the programmer must keep in mind while developing GPU-accelerated applications. What is allocated on CPUs is directly not accessible from GPUs and vice-versa. CUDA only offers low-level primitives for managing memory on the GPU, similar to languages like C. TNL helps the programmer with memory management by including helper classes aiming to alleviate the work needed to address memories between the devices.

\code{TNL::Container::Array} is a template container class for one-dimensional dynamic array. By specifying a type in a template, this class allows the programmer to choose where the data will reside. If the template argument is set to \code{TNL::Devices::Host}, the container behaves similar to \code{std::vector}, storing the data in the main memory. However, when the device is set to \code{TNL::Devices::Cuda}, data is stored in the GPU, and all of the operations need to be invoked with parallelism in mind to utilize that fact.

Data can be directly accessed only from a device where it was previously allocated. To read or write from a different device, copying of data must occur between memories. These cross-device operations are considerably expensive and should thus be used sparingly.

One common problem when developing GPU-accelerated programs with TNL is the ability to supply an instance of Array containers. Object instances cannot be passed to the kernel by reference, and every object must be deep-copied. This implementation detail brings significant performance overhead but also raises the question: How to mirror the changes back to the CPU copy of the object? A companion class is introduced to solve this question: \codecpp{TNL::Container::ArrayView}. This class implements the proxy design pattern, substituting \codecpp{TNL::Container::Array}, and allows the user to read and write into the array but permits the user from performing an operation, which may change memory allocation of the array, like resizing.
