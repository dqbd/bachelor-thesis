\section{CUDA programming model}

To simplify development on general-purpose GPUs, NVIDIA introduced \acrfull{cuda} programming model in November 2006. With CUDA, users can write applications exploiting the parallel nature of GPU using familiar high-level programming languages such as C/\CC.

The core idea behind \acrshort{cuda} is that parallel threads running on GPU are created and invoked by a coprocessor. This relationship between a CPU and GPU is modeled in CUDA as two separate entities: Host (CPU) and Device (GPU). A program is run by the host, which then launches parallel threads designed to be run on the device.

\acrshort{cuda} allows the programmer to define functions, called kernels, which can be executed multiple times in parallel by different CUDA threads. These threads need to be grouped into thread blocks, as these blocks are assigned to a specific SM limited in the number of processors. Thus, to invoke a kernel function, we need to pass additional parameters describing the number of thread blocks and the size of each thread block.

Internally, only a few threads can execute at once by a streaming multiprocessor. Threads inside a thread block are thus partitioned into groups of 32 called warps. A warp then executes a common instruction for all of its threads at the same time. This architecture is called \acrfull{simt}.

The threads can execute each instruction independently, as each thread in a warp has its registry state and instruction address counter. Thus the warps can freely branch and diverge. However, if a thread needs to take a path caused by a data-dependent branch, all other ones not on that same path are disabled, hindering the performance. Warps are at their peak efficiency when all of their threads execute code without branching.