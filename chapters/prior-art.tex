\section{Prior Art}

Significant work has been done in the past to optimize B-tree indexing capabilities. This chapter focuses on past work utilizing parallel computing to improve B-tree performance.

Bw-Tree introduced by Levandoski et al. \cite{bw-tree} from Microsoft Research is a B-Tree structure optimized for in-memory data processing and high concurrency access without using locks. To avoid locking, an indirection layer is introduced by maintaining a chain of records for each node. These records allow atomic updates of memory location of a node using a single compare-and-swap instruction.

Wang et al. \cite{openbw-tree} expanded the work done in \cite{bw-tree} by describing database specific implementation details regarding iterators and enabling non-unique key support.

Sewall et al. \cite{palm} introduced novel modifications to B+Tree operations in the proposed PALM technique. This technique uses the Bulk Synchronous Parallel model, where queries are grouped and the work is distributed among threads. This work also optimizes synchronization by avoiding barriers in favour of communicating of adjacent threads in a point-to-point manner. Latches are avoided under condition that all search queries have been completed before insertions and a node may be written by exactly one thread.

Previous projects related to GPU implementation of B-trees focused on query throughput. Fix et al. \cite{fix2011accelerating} measured substantial performance speedup compared to sequential CPU execution by running independent queries in each thread block. Until recently, the general approach for updating is either to perform such updates on the CPU or to rebuild the structure from scratch, which is the case of Fix et al. implementation.

Kim et al. proposed FAST \cite{fast}, a configurable high-performance tree optimized for SIMD and multi-core CPU and GPU systems. The structure can be configured towards the target hardware architecture by specifying the size of a cache line, SIMD register width and memory page size. Similar to Fix et al, only bulk creation and querying is supported, updates are done by rebuilding the tree.

Kaczmarski \cite{kaczmarski} utilized the relationship between a sorted list and demonstrated improved insertion time by presorting keys and proposing a novel bottom-up approach when constructing a tree. The work also explored several methods of key searching within a tree node.

Shahvarani et al. \cite{hb+tree} created a B+Tree variant utilizing the heterogeneous nature of the CPU and the GPU during a search operation. This approach has the benefit of removing the memory capacity restrictions, as the key-value pairs are residing on the host, whereas internal nodes are present on the device.

In Yan et al. \cite{harmonia}, instead of having the keys and children of a node in one instance, they are stored into separate arrays. Keys are stored in a one-dimensional array, and children are stored in a prefix-sum array. Storing keys and children into separate array improve memory locality and thus decreasing memory latency.

Awad et al. \cite{awad} proposed the most comprehensive GPU implementation of B-Tree to date. Their implementation supports concurrent queries (single, range, successor), insertion, and deletion. To achieve state-of-art performance, B-Link-Tree has been coupled with proactive splitting and Warp Cooperative Work Sharing as proposed by Ashkiani et al. \cite{ashkiani2018dynamic}.

